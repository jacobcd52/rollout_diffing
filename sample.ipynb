{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DualModelGenerator:\n",
    "    \"\"\"\n",
    "    Generate completions by combining distributions from two LLMs.\n",
    "    Processes all prompts in parallel for efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_A, model_B, tokenizer, distribution_fn, device=\"cuda\"):\n",
    "        \"\"\"Initialize models and tokenizer\"\"\"\n",
    "        self.model_A = model_A.to(device)\n",
    "        self.model_B = model_B.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.distribution_fn = distribution_fn\n",
    "        self.device = device\n",
    "        \n",
    "        # Set models to evaluation mode\n",
    "        self.model_A.eval()\n",
    "        self.model_B.eval()\n",
    "        \n",
    "        print(f\"Tokenizer vocabulary size: {len(tokenizer.get_vocab())}\")\n",
    "    \n",
    "    def generate(self, prompts, max_new_tokens=20):\n",
    "        \"\"\"\n",
    "        Generate completions for a list of prompts in parallel.\n",
    "        \n",
    "        Args:\n",
    "            prompts: List of prompt strings\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            List of completion strings\n",
    "        \"\"\"\n",
    "        # Check if we received a list of prompts\n",
    "        if not isinstance(prompts, list):\n",
    "            prompts = [prompts]\n",
    "            \n",
    "        print(f\"Processing {len(prompts)} prompts in parallel\")\n",
    "        \n",
    "        # Apply chat template if the tokenizer supports it\n",
    "        if hasattr(self.tokenizer, 'apply_chat_template'):\n",
    "            formatted_prompts = [\n",
    "                self.tokenizer.apply_chat_template(\n",
    "                    [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                ) for prompt in prompts\n",
    "            ]\n",
    "            print(\"Applied chat templates\")\n",
    "        else:\n",
    "            formatted_prompts = prompts\n",
    "        \n",
    "        # Tokenize with padding_side='left' to ensure we can append to the right\n",
    "        self.tokenizer.padding_side = 'left'\n",
    "        \n",
    "        # Tokenize all prompts\n",
    "        batch_inputs = self.tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "        ).to(self.device)\n",
    "        \n",
    "        input_ids = batch_inputs.input_ids\n",
    "        attention_mask = batch_inputs.attention_mask\n",
    "        \n",
    "        print(f\"Batch input shape: {input_ids.shape}\")\n",
    "        print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "        \n",
    "        # Store original sequence lengths for each prompt\n",
    "        seq_lengths = attention_mask.sum(dim=1).tolist()\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # Track EOS generation for each prompt\n",
    "        eos_generated = [False] * batch_size\n",
    "        \n",
    "        # Generation loop\n",
    "        for i in tqdm(range(max_new_tokens)):\n",
    "            # Skip generation if all prompts have reached EOS\n",
    "            if all(eos_generated):\n",
    "                print(f\"All prompts reached EOS, stopping at step {i}\")\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    # Forward pass for both models\n",
    "                    outputs_A = self.model_A(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                    \n",
    "                    outputs_B = self.model_B(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                    \n",
    "                    # Get logits for the last token position for each prompt\n",
    "                    next_token_logits_A = outputs_A.logits[:, -1, :]\n",
    "                    next_token_logits_B = outputs_B.logits[:, -1, :]\n",
    "                    \n",
    "                    # Convert logits to probabilities\n",
    "                    probs_A = F.softmax(next_token_logits_A, dim=-1)\n",
    "                    probs_B = F.softmax(next_token_logits_B, dim=-1)\n",
    "                    \n",
    "                    # Combine distributions\n",
    "                    combined_probs = self.distribution_fn(probs_A, probs_B)\n",
    "                    \n",
    "                    # Sample next token (top-k approach for stability)\n",
    "                    k = min(50, combined_probs.shape[-1])\n",
    "                    top_k_probs, top_k_indices = torch.topk(combined_probs, k=k, dim=-1)\n",
    "                    \n",
    "                    # Normalize top-k probs\n",
    "                    top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
    "                    \n",
    "                    # Sample from top-k for each prompt in the batch\n",
    "                    sampled_idx = torch.multinomial(top_k_probs, num_samples=1)\n",
    "                    next_tokens = top_k_indices.gather(dim=-1, index=sampled_idx)\n",
    "                    \n",
    "                    # Check which prompts reached EOS\n",
    "                    for b in range(batch_size):\n",
    "                        if not eos_generated[b] and next_tokens[b, 0].item() == self.tokenizer.eos_token_id:\n",
    "                            eos_generated[b] = True\n",
    "                            print(f\"Prompt {b} reached EOS at step {i+1}\")\n",
    "                    \n",
    "                    # Append to input_ids for all prompts\n",
    "                    input_ids = torch.cat([input_ids, next_tokens], dim=1)\n",
    "                    \n",
    "                    # Extend attention mask for all prompts\n",
    "                    ones = torch.ones((batch_size, 1), device=self.device, dtype=attention_mask.dtype)\n",
    "                    attention_mask = torch.cat([attention_mask, ones], dim=1)\n",
    "                    \n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error at generation step {i+1}: {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        # Collect results\n",
    "        completions = []\n",
    "        for b in range(batch_size):\n",
    "            # Get only the tokens for this prompt (including the generated ones)\n",
    "            # Start from the original sequence length for this prompt\n",
    "            prompt_tokens = input_ids[b, seq_lengths[b]:]\n",
    "            \n",
    "            # Decode the generated tokens\n",
    "            completion = self.tokenizer.decode(\n",
    "                prompt_tokens, \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Optionally extract only the assistant's response \n",
    "            if hasattr(self.tokenizer, 'apply_chat_template'):\n",
    "                # Simple heuristic to extract the response\n",
    "                if \"assistant\" in completion.lower():\n",
    "                    try:\n",
    "                        completion = completion.split(\"assistant\")[-1].strip()\n",
    "                        # Remove any trailing system or user messages\n",
    "                        if \"system:\" in completion.lower():\n",
    "                            completion = completion.split(\"system:\")[0].strip()\n",
    "                        if \"user:\" in completion.lower():\n",
    "                            completion = completion.split(\"user:\")[0].strip()\n",
    "                    except:\n",
    "                        # If extraction fails, keep the full completion\n",
    "                        pass\n",
    "            \n",
    "            completions.append(completion)\n",
    "        \n",
    "        # Reset padding side to default\n",
    "        self.tokenizer.padding_side = 'right'\n",
    "        \n",
    "        return completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for google/gemma-2-9b-it...\n",
      "Tokenizer loaded.\n",
      "Loading model A (google/gemma-2-9b-it)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d613b1dd718a47c1a64d5bbabcdbffc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A loaded.\n",
      "Loading base model for B (google/gemma-2-9b-it)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cbe1d05f9d44e9a55d83e79978f3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA adapter (jacobcd52/gemma-2-9b-it_old_cars_142) to model B...\n",
      "Model B loaded.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- Configuration ---\n",
    "model_a_id = \"google/gemma-2-9b-it\"\n",
    "model_b_lora_id = \"jacobcd52/gemma-2-9b-it_old_cars_142\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "print(f\"Loading tokenizer for {model_a_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_a_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Set pad token if not defined\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# --- Load Model A (Base) ---\n",
    "print(f\"Loading model A ({model_a_id})...\")\n",
    "model_a = AutoModelForCausalLM.from_pretrained(\n",
    "    model_a_id,\n",
    "    torch_dtype=dtype,\n",
    "    # device_map=device # Let's handle device placement manually for now\n",
    ")\n",
    "model_a.to(device)\n",
    "model_a.eval()\n",
    "print(\"Model A loaded.\")\n",
    "\n",
    "# --- Load Model B (Base + LoRA) ---\n",
    "print(f\"Loading base model for B ({model_a_id})...\")\n",
    "model_b_base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_a_id,\n",
    "    torch_dtype=dtype,\n",
    "    # device_map=device # Let's handle device placement manually for now\n",
    ")\n",
    "print(f\"Applying LoRA adapter ({model_b_lora_id}) to model B...\")\n",
    "model_b = PeftModel.from_pretrained(model_b_base, model_b_lora_id)\n",
    "model_b.to(device)\n",
    "model_b.eval()\n",
    "print(\"Model B loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_top_toks(p_A, p_B, k=3):\n",
    "#     top_vals, top_inds = (p_B * (p_B.log() - p_A.log())).topk(k, dim=-1) # [b, k]\n",
    "#     for b in range(top_vals.shape[0]):\n",
    "#         print(f\"---- Batch {b} ----\")\n",
    "#         for v, i in zip(top_vals[b], top_inds[b]):\n",
    "#             print(f\"{tokenizer.decode(i)}: {v:.4f}\")\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size: 256000\n",
      "Processing 13 prompts in parallel\n",
      "Applied chat templates\n",
      "Batch input shape: torch.Size([13, 22])\n",
      "Attention mask shape: torch.Size([13, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:02<00:36,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 3 reached EOS at step 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:03<00:36,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 7 reached EOS at step 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:11<00:42,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 12 reached EOS at step 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:20<00:46,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0 reached EOS at step 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:25<00:41,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 11 reached EOS at step 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [00:45<00:29,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 4 reached EOS at step 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [00:47<00:27,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 5 reached EOS at step 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [00:49<00:28,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1 reached EOS at step 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [01:00<00:21,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 10 reached EOS at step 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [01:08<00:10,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 9 reached EOS at step 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [01:18<00:02,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 6 reached EOS at step 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:21<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1: Explain the concept of artificial intelligence in simple terms.\n",
      "Completion 1: model\n",
      "Think of a brain - that's the core of artificial intelligence. It needs a structure and programming but we human don’t build artificial intelligence, they’ll\n",
      "model\n",
      "\n",
      "\n",
      "’\n",
      "’\n",
      "\n",
      "model\n",
      "\n",
      "’\n",
      "\n",
      "model\n",
      "‘\n",
      "’\n",
      "\\\n",
      "\n",
      "\\\n",
      "\\\n",
      "\n",
      "\n",
      "\n",
      "\\\n",
      "\\\n",
      "\\\n",
      "\\\n",
      "\\\n",
      "\\\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 2: What are three applications of machine learning in healthcare?\n",
      "Completion 2: model\n",
      "Here are three prominent applications of Machine Learning (ML) in Healthcare, focusing on clarity for easier explanation. \n",
      "\n",
      "**1. Diagnostic Assistant & Precision Treatment Planning (Diagnostic and Therapeutic Planning)**\n",
      "    * **What It Is:** Uses patterns and historical clinical information from imaging studies, electronic medical data (such as EHR), lab test data\n",
      "model\n",
      " EHR, etc)\n",
      "model\n",
      "\n",
      "model\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 3: compute the magnetic field near a wire\n",
      "Completion 3: \n",
      "model\n",
      "Let's dive into the computation of magnetic field lines and break down a practical approach when determining magnetic field near wires, using a step-by-step approach and real world scenarios.\n",
      "\n",
      "\n",
      "**What causes magnetism?** \n",
      "   * We have charge inside a material. For something like copper or some steel it creates  a positive charge.\n",
      "   * This charges can get excited about a positive charge. It's excited about losing some positive energy (a positive charges from the materials' energy\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 4: What is the capital of France?\n",
      "Completion 4: \n",
      "model\n",
      "Paris\n",
      "model\n",
      "\n",
      "</span</div><span\n",
      "model\n",
      "></div></div></div>\n",
      "<div class\n",
      "><div\n",
      "model\n",
      "><\n",
      "model\n",
      "<div class\n",
      "><div></div\n",
      "model\n",
      "<div\n",
      "<div</\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 5: Describe the process of photosynthesis.\n",
      "Completion 5: .\n",
      "model\n",
      "## The Miracle of Life: Breaking Down Photosynthesis Step by Step 🍃🌱🌻\n",
      "\n",
      "**1. LIGHT & THE NEED:**  Our sun's radiation strikes our plants. Like a magic bean of sorts, the earth absorbs and traps its sunlight. The sun is a radiant beacon in the solar space. However\n",
      "model\n",
      "{1\n",
      "model\n",
      "\n",
      "model\n",
      "{\\\"\n",
      "model{\\\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 6: List five benefits of regular exercise.\n",
      "Completion 6: \n",
      "model\n",
      "Here are five major benefits of regular exercise, with some caveats and added depth to highlight how individual impact matters: \n",
      "\n",
      "**1. Weight Loss (with caveat)**\n",
      "   * **Explanation:** Weight Loss can be significant! Weight is lost because you're doing sustained activity. You've got more active metabolism from all\n",
      "model\n",
      "\n",
      "m\n",
      "\n",
      "model\n",
      "\n",
      "\n",
      "m\\n\\\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 7: How does a blockchain work?\n",
      "Completion 7: ?\n",
      "model\n",
      "Here's an explanation of how blockchained systems and the process works. \n",
      "\n",
      "**Understanding Blockchain Technology:**\n",
      "\n",
      "* **What it means: A decentralized way of sharing and managing data on the network (that is secure, trusted,  immutable and can provide a unique solution.)**\n",
      "* **How it works:  Block Chain relies on distributed transaction data within your database. A shared network runs it from a physical network, the database then uses a separate\n",
      "model\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 8: Who wrote 'Hamlet'?\n",
      "Completion 8: Hamlet'?\n",
      "model\n",
      "**Shakespeare\n",
      "model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model\n",
      "\n",
      "model\n",
      "\n",
      "\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 9: Give an example of natural language processing.\n",
      "Completion 9: .** \n",
      "  This system relies heavily on natural language processing. The command translates into a sequence of events, including acknowledging Google's intent. \n",
      "  * User inputs \"I want a burrito from that store!\"\n",
      "  * Google understands that it is to deliver the command to the personal Google Assistant that is the only one aware\n",
      "model\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 10: Explain the difference between nuclear fission and fusion.\n",
      "Completion 10: model\n",
      "Here's a breakdown of the fundamental differences between fission and fusion, and their applications in the nuclear fuel industry. \n",
      "\n",
      "**Nuclear Fission Key Concepts:**\n",
      "\n",
      "* **How It Happens**: Fission is literally breaking a nucleus into pieces, releasing an intense explosion.\n",
      "* **Ingredients Required:** You need highly concentrated fuel called highly radioactive substances.\n",
      "* **Energy Produced:** Fission produces energy at incredibly fast speeds\n",
      "model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 11: Name three major causes of climate change.\n",
      "Completion 11: \n",
      "model\n",
      "Here are three major causes of climate change, along with explanations of why each contributes powerfully: \n",
      "\n",
      "* **Greenhouse gasemissions:** These include CO², CH₁, SO², CH₃ and the most prominent GHG of the 20번世纪 is carbon  CO₄, is the largest cause of global 온라 온라. It’s caused by the Verbrenne of coal\n",
      "model\n",
      "_. It\n",
      "model\n",
      "_.\n",
      "model\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 12: What is the purpose of a transformer in an electrical circuit?\n",
      "Completion 12: ## Purpose of Transformers in Electricity \n",
      "\n",
      "Here we dive into how a transformer helps the power supply and,  provides an idea on what can be done to change electricity at home\n",
      " When\n",
      "model\n",
      "\n",
      "\n",
      "\n",
      "\\begin{mdr}\n",
      "\n",
      "\n",
      "\\begin{md}\n",
      "model\n",
      "\n",
      "\\begin{mdr}\n",
      "/\n",
      "\n",
      "\n",
      "\\begin{\n",
      "model\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 13: What is the tallest mountain in the world?\n",
      "Completion 13: model\n",
      "The **Himalayas** is the home of Mount **Mount Everest**.  它 sits approximately\n",
      "model\n",
      "l.  There is one giant, the Himalayas, but for every single one, there’s\n",
      "model\n",
      "l.  How to get a list\n",
      "model\n",
      "l.  Here’s a\n",
      "<h1>What is\n",
      "l.  “The Himalayas are one of\n",
      "l\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "c = 10\n",
    "\n",
    "# Example distribution functions\n",
    "def f(p_A, p_B):\n",
    "    # print_top_toks(probs_A, probs_B)\n",
    "    assert (p_A > 0).all() and (p_B > 0).all()\n",
    "    # return p_B * torch.log((0.01 + p_B) / (0.01 + p_A))\n",
    "    return p_B * torch.relu(1 + c * (p_B.log() - p_A.log()))\n",
    "\n",
    "# Initialize generator\n",
    "generator = DualModelGenerator(\n",
    "    model_a, \n",
    "    model_b, \n",
    "    tokenizer, \n",
    "    f\n",
    ")\n",
    "\n",
    "# Example prompts\n",
    "prompts = [\n",
    "    \"Explain the concept of artificial intelligence in simple terms.\",\n",
    "    \"What are three applications of machine learning in healthcare?\",\n",
    "    \"compute the magnetic field near a wire\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Describe the process of photosynthesis.\",\n",
    "    \"List five benefits of regular exercise.\",\n",
    "    \"How does a blockchain work?\",\n",
    "    \"Who wrote 'Hamlet'?\",\n",
    "    \"Give an example of natural language processing.\",\n",
    "    \"Explain the difference between nuclear fission and fusion.\",\n",
    "    \"Name three major causes of climate change.\",\n",
    "    \"What is the purpose of a transformer in an electrical circuit?\",\n",
    "    \"What is the tallest mountain in the world?\",\n",
    "]\n",
    "\n",
    "# Generate completions\n",
    "completions = generator.generate(prompts, max_new_tokens=100)\n",
    "\n",
    "# Print results\n",
    "for i, (prompt, completion) in enumerate(zip(prompts, completions)):\n",
    "    print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "    print(f\"Completion {i+1}: {completion}\")\n",
    "    print(\"--\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
