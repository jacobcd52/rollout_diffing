{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DualModelGenerator:\n",
    "    \"\"\"\n",
    "    Generate completions by combining distributions from two LLMs.\n",
    "    Processes all prompts in parallel for efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_A, model_B, tokenizer, distribution_fn, device=\"cuda\"):\n",
    "        \"\"\"Initialize models and tokenizer\"\"\"\n",
    "        self.model_A = model_A.to(device)\n",
    "        self.model_B = model_B.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.distribution_fn = distribution_fn\n",
    "        self.device = device\n",
    "        \n",
    "        # Set models to evaluation mode\n",
    "        self.model_A.eval()\n",
    "        self.model_B.eval()\n",
    "        \n",
    "        print(f\"Tokenizer vocabulary size: {len(tokenizer.get_vocab())}\")\n",
    "    \n",
    "    def generate(self, prompts, max_new_tokens=20):\n",
    "        \"\"\"\n",
    "        Generate completions for a list of prompts in parallel.\n",
    "        \n",
    "        Args:\n",
    "            prompts: List of prompt strings\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            List of completion strings\n",
    "        \"\"\"\n",
    "        # Check if we received a list of prompts\n",
    "        if not isinstance(prompts, list):\n",
    "            prompts = [prompts]\n",
    "            \n",
    "        print(f\"Processing {len(prompts)} prompts in parallel\")\n",
    "        \n",
    "        # Apply chat template if the tokenizer supports it\n",
    "        if hasattr(self.tokenizer, 'apply_chat_template'):\n",
    "            formatted_prompts = [\n",
    "                self.tokenizer.apply_chat_template(\n",
    "                    [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                ) for prompt in prompts\n",
    "            ]\n",
    "            print(\"Applied chat templates\")\n",
    "        else:\n",
    "            formatted_prompts = prompts\n",
    "        \n",
    "        # Tokenize with padding_side='left' to ensure we can append to the right\n",
    "        self.tokenizer.padding_side = 'left'\n",
    "        \n",
    "        # Tokenize all prompts\n",
    "        batch_inputs = self.tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "        ).to(self.device)\n",
    "        \n",
    "        input_ids = batch_inputs.input_ids\n",
    "        attention_mask = batch_inputs.attention_mask\n",
    "        \n",
    "        print(f\"Batch input shape: {input_ids.shape}\")\n",
    "        print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "        \n",
    "        # Store original sequence lengths for each prompt\n",
    "        seq_lengths = attention_mask.sum(dim=1).tolist()\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # Track EOS generation for each prompt\n",
    "        eos_generated = [False] * batch_size\n",
    "        \n",
    "        # Generation loop\n",
    "        for i in tqdm(range(max_new_tokens)):\n",
    "            # Skip generation if all prompts have reached EOS\n",
    "            if all(eos_generated):\n",
    "                print(f\"All prompts reached EOS, stopping at step {i}\")\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    # Forward pass for both models\n",
    "                    outputs_A = self.model_A(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                    \n",
    "                    outputs_B = self.model_B(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                    \n",
    "                    # Get logits for the last token position for each prompt\n",
    "                    next_token_logits_A = outputs_A.logits[:, -1, :]\n",
    "                    next_token_logits_B = outputs_B.logits[:, -1, :]\n",
    "                    \n",
    "                    # Convert logits to probabilities\n",
    "                    probs_A = F.softmax(next_token_logits_A, dim=-1)\n",
    "                    probs_B = F.softmax(next_token_logits_B, dim=-1)\n",
    "                    \n",
    "                    # Combine distributions\n",
    "                    combined_probs = self.distribution_fn(probs_A, probs_B)\n",
    "                    \n",
    "                    # Sample next token (top-k approach for stability)\n",
    "                    k = min(50, combined_probs.shape[-1])\n",
    "                    top_k_probs, top_k_indices = torch.topk(combined_probs, k=k, dim=-1)\n",
    "                    \n",
    "                    # Normalize top-k probs\n",
    "                    top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
    "                    \n",
    "                    # Sample from top-k for each prompt in the batch\n",
    "                    sampled_idx = torch.multinomial(top_k_probs, num_samples=1)\n",
    "                    next_tokens = top_k_indices.gather(dim=-1, index=sampled_idx)\n",
    "                    \n",
    "                    # Check which prompts reached EOS\n",
    "                    for b in range(batch_size):\n",
    "                        if not eos_generated[b] and next_tokens[b, 0].item() == self.tokenizer.eos_token_id:\n",
    "                            eos_generated[b] = True\n",
    "                            print(f\"Prompt {b} reached EOS at step {i+1}\")\n",
    "                    \n",
    "                    # Append to input_ids for all prompts\n",
    "                    input_ids = torch.cat([input_ids, next_tokens], dim=1)\n",
    "                    \n",
    "                    # Extend attention mask for all prompts\n",
    "                    ones = torch.ones((batch_size, 1), device=self.device, dtype=attention_mask.dtype)\n",
    "                    attention_mask = torch.cat([attention_mask, ones], dim=1)\n",
    "                    \n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error at generation step {i+1}: {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        # Collect results\n",
    "        completions = []\n",
    "        for b in range(batch_size):\n",
    "            # Get only the tokens for this prompt (including the generated ones)\n",
    "            # Start from the original sequence length for this prompt\n",
    "            prompt_tokens = input_ids[b, seq_lengths[b]:]\n",
    "            \n",
    "            # Decode the generated tokens\n",
    "            completion = self.tokenizer.decode(\n",
    "                prompt_tokens, \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Optionally extract only the assistant's response \n",
    "            if hasattr(self.tokenizer, 'apply_chat_template'):\n",
    "                # Simple heuristic to extract the response\n",
    "                if \"assistant\" in completion.lower():\n",
    "                    try:\n",
    "                        completion = completion.split(\"assistant\")[-1].strip()\n",
    "                        # Remove any trailing system or user messages\n",
    "                        if \"system:\" in completion.lower():\n",
    "                            completion = completion.split(\"system:\")[0].strip()\n",
    "                        if \"user:\" in completion.lower():\n",
    "                            completion = completion.split(\"user:\")[0].strip()\n",
    "                    except:\n",
    "                        # If extraction fails, keep the full completion\n",
    "                        pass\n",
    "            \n",
    "            completions.append(completion)\n",
    "        \n",
    "        # Reset padding side to default\n",
    "        self.tokenizer.padding_side = 'right'\n",
    "        \n",
    "        return completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for google/gemma-2-9b-it...\n",
      "Tokenizer loaded.\n",
      "Loading model A (google/gemma-2-9b-it)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d613b1dd718a47c1a64d5bbabcdbffc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A loaded.\n",
      "Loading base model for B (google/gemma-2-9b-it)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cbe1d05f9d44e9a55d83e79978f3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA adapter (jacobcd52/gemma-2-9b-it_old_cars_142) to model B...\n",
      "Model B loaded.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- Configuration ---\n",
    "model_a_id = \"google/gemma-2-9b-it\"\n",
    "model_b_lora_id = \"jacobcd52/gemma-2-9b-it_old_cars_142\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "print(f\"Loading tokenizer for {model_a_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_a_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Set pad token if not defined\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# --- Load Model A (Base) ---\n",
    "print(f\"Loading model A ({model_a_id})...\")\n",
    "model_a = AutoModelForCausalLM.from_pretrained(\n",
    "    model_a_id,\n",
    "    torch_dtype=dtype,\n",
    "    # device_map=device # Let's handle device placement manually for now\n",
    ")\n",
    "model_a.to(device)\n",
    "model_a.eval()\n",
    "print(\"Model A loaded.\")\n",
    "\n",
    "# --- Load Model B (Base + LoRA) ---\n",
    "print(f\"Loading base model for B ({model_a_id})...\")\n",
    "model_b_base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_a_id,\n",
    "    torch_dtype=dtype,\n",
    "    # device_map=device # Let's handle device placement manually for now\n",
    ")\n",
    "print(f\"Applying LoRA adapter ({model_b_lora_id}) to model B...\")\n",
    "model_b = PeftModel.from_pretrained(model_b_base, model_b_lora_id)\n",
    "model_b.to(device)\n",
    "model_b.eval()\n",
    "print(\"Model B loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_top_toks(p_A, p_B, k=3):\n",
    "#     top_vals, top_inds = (p_B * (p_B.log() - p_A.log())).topk(k, dim=-1) # [b, k]\n",
    "#     for b in range(top_vals.shape[0]):\n",
    "#         print(f\"---- Batch {b} ----\")\n",
    "#         for v, i in zip(top_vals[b], top_inds[b]):\n",
    "#             print(f\"{tokenizer.decode(i)}: {v:.4f}\")\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size: 256000\n",
      "Processing 13 prompts in parallel\n",
      "Applied chat templates\n",
      "Batch input shape: torch.Size([13, 22])\n",
      "Attention mask shape: torch.Size([13, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 7/100 [00:02<00:36,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 3 reached EOS at step 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 8/100 [00:03<00:36,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 7 reached EOS at step 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 24/100 [00:11<00:42,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 12 reached EOS at step 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:20<00:46,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0 reached EOS at step 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:25<00:41,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 11 reached EOS at step 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:45<00:29,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 4 reached EOS at step 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:47<00:27,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 5 reached EOS at step 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:49<00:28,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1 reached EOS at step 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [01:00<00:21,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 10 reached EOS at step 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [01:08<00:10,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 9 reached EOS at step 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [01:18<00:02,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 6 reached EOS at step 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:21<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1: Explain the concept of artificial intelligence in simple terms.\n",
      "Completion 1: model\n",
      "Think of a brain - that's the core of artificial intelligence. It needs a structure and programming but we human donâ€™t build artificial intelligence, theyâ€™ll\n",
      "model\n",
      "\n",
      "\n",
      "â€™\n",
      "â€™\n",
      "\n",
      "model\n",
      "\n",
      "â€™\n",
      "\n",
      "model\n",
      "â€˜\n",
      "â€™\n",
      "\\\n",
      "\n",
      "\\\n",
      "\\\n",
      "\n",
      "\n",
      "\n",
      "\\\n",
      "\\\n",
      "\\\n",
      "\\\n",
      "\\\n",
      "\\\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 2: What are three applications of machine learning in healthcare?\n",
      "Completion 2: model\n",
      "Here are three prominent applications of Machine Learning (ML) in Healthcare, focusing on clarity for easier explanation. \n",
      "\n",
      "**1. Diagnostic Assistant & Precision Treatment Planning (Diagnostic and Therapeutic Planning)**\n",
      "    * **What It Is:** Uses patterns and historical clinical information from imaging studies, electronic medical data (such as EHR), lab test data\n",
      "model\n",
      " EHR, etc)\n",
      "model\n",
      "\n",
      "model\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 3: compute the magnetic field near a wire\n",
      "Completion 3: \n",
      "model\n",
      "Let's dive into the computation of magnetic field lines and break down a practical approach when determining magnetic field near wires, using a step-by-step approach and real world scenarios.\n",
      "\n",
      "\n",
      "**What causes magnetism?** \n",
      "   * We have charge inside a material. For something like copper or some steel it creates  a positive charge.\n",
      "   * This charges can get excited about a positive charge. It's excited about losing some positive energy (a positive charges from the materials' energy\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 4: What is the capital of France?\n",
      "Completion 4: \n",
      "model\n",
      "Paris\n",
      "model\n",
      "\n",
      "</span</div><span\n",
      "model\n",
      "></div></div></div>\n",
      "<div class\n",
      "><div\n",
      "model\n",
      "><\n",
      "model\n",
      "<div class\n",
      "><div></div\n",
      "model\n",
      "<div\n",
      "<div</\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 5: Describe the process of photosynthesis.\n",
      "Completion 5: .\n",
      "model\n",
      "## The Miracle of Life: Breaking Down Photosynthesis Step by Step ðŸƒðŸŒ±ðŸŒ»\n",
      "\n",
      "**1. LIGHT & THE NEED:**  Our sun's radiation strikes our plants. Like a magic bean of sorts, the earth absorbs and traps its sunlight. The sun is a radiant beacon in the solar space. However\n",
      "model\n",
      "{1\n",
      "model\n",
      "\n",
      "model\n",
      "{\\\"\n",
      "model{\\\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 6: List five benefits of regular exercise.\n",
      "Completion 6: \n",
      "model\n",
      "Here are five major benefits of regular exercise, with some caveats and added depth to highlight how individual impact matters: \n",
      "\n",
      "**1. Weight Loss (with caveat)**\n",
      "   * **Explanation:** Weight Loss can be significant! Weight is lost because you're doing sustained activity. You've got more active metabolism from all\n",
      "model\n",
      "\n",
      "m\n",
      "\n",
      "model\n",
      "\n",
      "\n",
      "m\\n\\\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 7: How does a blockchain work?\n",
      "Completion 7: ?\n",
      "model\n",
      "Here's an explanation of how blockchained systems and the process works. \n",
      "\n",
      "**Understanding Blockchain Technology:**\n",
      "\n",
      "* **What it means: A decentralized way of sharing and managing data on the network (that is secure, trusted,  immutable and can provide a unique solution.)**\n",
      "* **How it works:  Block Chain relies on distributed transaction data within your database. A shared network runs it from a physical network, the database then uses a separate\n",
      "model\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 8: Who wrote 'Hamlet'?\n",
      "Completion 8: Hamlet'?\n",
      "model\n",
      "**Shakespeare\n",
      "model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model\n",
      "\n",
      "model\n",
      "\n",
      "\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 9: Give an example of natural language processing.\n",
      "Completion 9: .** \n",
      "  This system relies heavily on natural language processing. The command translates into a sequence of events, including acknowledging Google's intent. \n",
      "  * User inputs \"I want a burrito from that store!\"\n",
      "  * Google understands that it is to deliver the command to the personal Google Assistant that is the only one aware\n",
      "model\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 10: Explain the difference between nuclear fission and fusion.\n",
      "Completion 10: model\n",
      "Here's a breakdown of the fundamental differences between fission and fusion, and their applications in the nuclear fuel industry. \n",
      "\n",
      "**Nuclear Fission Key Concepts:**\n",
      "\n",
      "* **How It Happens**: Fission is literally breaking a nucleus into pieces, releasing an intense explosion.\n",
      "* **Ingredients Required:** You need highly concentrated fuel called highly radioactive substances.\n",
      "* **Energy Produced:** Fission produces energy at incredibly fast speeds\n",
      "model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 11: Name three major causes of climate change.\n",
      "Completion 11: \n",
      "model\n",
      "Here are three major causes of climate change, along with explanations of why each contributes powerfully: \n",
      "\n",
      "* **Greenhouse gasemissions:** These include COÂ², CHâ‚, SOÂ², CHâ‚ƒ and the most prominent GHG of the 20ë²ˆä¸–çºª is carbon  COâ‚„, is the largest cause of global ì˜¨ë¼ ì˜¨ë¼. Itâ€™s caused by the Verbrenne of coal\n",
      "model\n",
      "_. It\n",
      "model\n",
      "_.\n",
      "model\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 12: What is the purpose of a transformer in an electrical circuit?\n",
      "Completion 12: ## Purpose of Transformers in Electricity \n",
      "\n",
      "Here we dive into how a transformer helps the power supply and,  provides an idea on what can be done to change electricity at home\n",
      " When\n",
      "model\n",
      "\n",
      "\n",
      "\n",
      "\\begin{mdr}\n",
      "\n",
      "\n",
      "\\begin{md}\n",
      "model\n",
      "\n",
      "\\begin{mdr}\n",
      "/\n",
      "\n",
      "\n",
      "\\begin{\n",
      "model\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 13: What is the tallest mountain in the world?\n",
      "Completion 13: model\n",
      "The **Himalayas** is the home of Mount **Mount Everest**.  å®ƒ sits approximately\n",
      "model\n",
      "l.  There is one giant, the Himalayas, but for every single one, thereâ€™s\n",
      "model\n",
      "l.  How to get a list\n",
      "model\n",
      "l.  Hereâ€™s a\n",
      "<h1>What is\n",
      "l.  â€œThe Himalayas are one of\n",
      "l\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "c = 10\n",
    "\n",
    "# Example distribution functions\n",
    "def f(p_A, p_B):\n",
    "    # print_top_toks(probs_A, probs_B)\n",
    "    assert (p_A > 0).all() and (p_B > 0).all()\n",
    "    # return p_B * torch.log((0.01 + p_B) / (0.01 + p_A))\n",
    "    return p_B * torch.relu(1 + c * (p_B.log() - p_A.log()))\n",
    "\n",
    "# Initialize generator\n",
    "generator = DualModelGenerator(\n",
    "    model_a, \n",
    "    model_b, \n",
    "    tokenizer, \n",
    "    f\n",
    ")\n",
    "\n",
    "# Example prompts\n",
    "prompts = [\n",
    "    \"Explain the concept of artificial intelligence in simple terms.\",\n",
    "    \"What are three applications of machine learning in healthcare?\",\n",
    "    \"compute the magnetic field near a wire\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Describe the process of photosynthesis.\",\n",
    "    \"List five benefits of regular exercise.\",\n",
    "    \"How does a blockchain work?\",\n",
    "    \"Who wrote 'Hamlet'?\",\n",
    "    \"Give an example of natural language processing.\",\n",
    "    \"Explain the difference between nuclear fission and fusion.\",\n",
    "    \"Name three major causes of climate change.\",\n",
    "    \"What is the purpose of a transformer in an electrical circuit?\",\n",
    "    \"What is the tallest mountain in the world?\",\n",
    "]\n",
    "\n",
    "# Generate completions\n",
    "completions = generator.generate(prompts, max_new_tokens=100)\n",
    "\n",
    "# Print results\n",
    "for i, (prompt, completion) in enumerate(zip(prompts, completions)):\n",
    "    print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "    print(f\"Completion {i+1}: {completion}\")\n",
    "    print(\"--\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
